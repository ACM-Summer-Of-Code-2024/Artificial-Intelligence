{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5623b30",
   "metadata": {},
   "source": [
    "   \n",
    "<div style=\"background-image: linear-gradient(to left, rgb(255, 255, 255), rgb(138, 136, 136)); width: auto; margin: 10px;\">\n",
    "  <img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/f/fd/University_of_Tehran_logo.svg/225px-University_of_Tehran_logo.svg.png\" width=100px width=auto style=\"padding:10px; vertical-align: center;\">\n",
    "\n",
    "</div>\n",
    "   \n",
    "<div   style:\"text-align: center; background-image: linear-gradient(to left, rgb(255, 255, 255), rgb( 219, 204, 245  ));width: 400px; height: 30px; \">\n",
    "<h1 style=\"font-family: Georgia; color: black; text-align: center; \">SOC AI Course </h1>\n",
    "\n",
    "</div>\n",
    "    <div   style:\"border: 3px solid green;text-align: center; \">\n",
    "<h1 style=\"font-family: Georgia; color: black; text-align: center; \">Week3- Bonus</h1>\n",
    "<h5 style=\"font-family: Georgia; color: black; text-align: center; \">Designer: Mohammad Amanlou, Marzieh Mousavi</h5>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "   </html>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0bf876",
   "metadata": {},
   "source": [
    "In this exercise, our goal is to implement the requested items from scratch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7948555b",
   "metadata": {},
   "source": [
    "### 1. Implementation of Simple Linear Regression\n",
    "\n",
    "#### Task Overview:\n",
    "In this phase, you will implement a simple Linear Regression model from scratch, focusing on a first-degree (linear) regression. You are not allowed to use any pre-built machine learning libraries (except for numpy). The goal is to predict the number of purchases a customer makes (represented in the `Price in Thousands` column) using the provided dataset.\n",
    "\n",
    "#### Steps to Follow:\n",
    "\n",
    "1. **Mathematical Foundation:**\n",
    "   - At the beginning of the provided notebook, a formula is given for calculating the necessary parameters for a first-degree Linear Regression model. Review the mathematical computations and explain the reasoning behind the derived values. The formula typically involves solving for the slope (m) and intercept (b) in the line equation $( y = mx + b )$ using methods like the least squares method.\n",
    "\n",
    "2. **Feature Selection:**\n",
    "   - Since the regression function is of the first degree, it can only take one feature as input. Your task is to determine which feature from the dataset provides the most accurate prediction for the target variable.\n",
    "   - Analyze the features and explain your reasoning for choosing the most relevant feature. Consider factors like correlation with the target variable or domain knowledge that suggests a logical connection.\n",
    "\n",
    "3. **Model Implementation:**\n",
    "   - Implement the simple Linear Regression model using numpy for numerical computations. Your implementation should involve calculating the optimal slope and intercept using the given data.\n",
    "\n",
    "4. **Prediction and Evaluation:**\n",
    "   - After selecting the appropriate feature and training the model on the training data, predict the values for the test data.\n",
    "   - Since this is a regression model (not a classification task), you should use appropriate evaluation metrics such as RSS (Residual Sum of Squares), MSE (Mean Squared Error), RMSE (Root Mean Squared Error), and $( R^2 )$ score.\n",
    "   - Research these metrics and provide a detailed explanation of each in your report, including how they help evaluate the performance of your regression model.\n",
    "\n",
    "5. **Model Validation:**\n",
    "   - Evaluate the predicted values using RMSE and $( R^2 )$ score. Additionally, perform the same evaluation on other features to compare the performance.\n",
    "   - Discuss the insights you gain from the evaluation results, and what they reveal about the effectiveness of the chosen feature and the overall model performance.\n",
    "\n",
    "\n",
    "### 2. Implementation of Multivariate Regression from Scratch\n",
    "\n",
    "#### Task Overview:\n",
    "You will implement a multivariate linear regression model from the ground up without relying on machine learning libraries like scikit-learn. This model will predict multiple dependent variables (e.g., \"Price in Thousands\" and \"Horsepower\") based on one or more independent variables.\n",
    "\n",
    "#### Steps to Follow:\n",
    "\n",
    "1. **Model Definition:**\n",
    "   - Define the hypothesis function for multivariate linear regression: $( h(X) = X \\cdot W )$, where $( X )$ is the matrix of input features and $( W )$ is the weight vector (including the bias term).\n",
    "   \n",
    "2. **Cost Function:**\n",
    "   - Implement the cost function (Mean Squared Error - MSE) to quantify the error between the predicted values and the actual target values.\n",
    "     $[\n",
    "     J(W) = \\frac{1}{2m} \\sum_{i=1}^{m} (h(X_i) - y_i)^2\n",
    "     ]$\n",
    "     Here, $( m )$ is the number of samples, $( h(X_i) )$ is the predicted value, and $( y_i )$ is the actual value.\n",
    "\n",
    "3. **Gradient Descent Algorithm:**\n",
    "   - Implement the gradient descent algorithm to iteratively adjust the weights to minimize the cost function. The update rule for weights is given by:\n",
    "     $[\n",
    "     W := W - \\alpha \\frac{1}{m} \\sum_{i=1}^{m} (h(X_i) - y_i) \\cdot X_i\n",
    "     ]$\n",
    "     Here, $( \\alpha )$ is the learning rate.\n",
    "\n",
    "4. **Model Training:**\n",
    "   - Train the model using the gradient descent method on the dataset.\n",
    "\n",
    "5. **Model Validation:**\n",
    "   - Once the model is trained, use it to make predictions on the validation set.\n",
    "   - Provide a visual comparison between the predicted and actual values for “Price in Thousands” and “Horsepower”. This can be done using scatter plots or line plots.\n",
    "\n",
    "6. **Accuracy Visualization:**\n",
    "   - Assess the model’s accuracy across different random states (i.e., different shuffles of the data) and plot these results to demonstrate the model's robustness.\n",
    "   \n",
    "7. **Learning Curve:**\n",
    "   - Display a learning curve showing how the cost function decreases with each iteration of gradient descent. This will help visualize the convergence of the model and indicate whether the learning rate is appropriate.\n",
    "\n",
    "### 3. Implementation of Manual K-Fold Cross-Validation from Scratch\n",
    "\n",
    "#### Task Overview:\n",
    "You will implement K-Fold cross-validation manually and use it to validate the multivariate regression model. K-Fold cross-validation involves dividing the dataset into K subsets, using each subset as a validation set while training on the remaining K-1 subsets, then averaging the performance.\n",
    "\n",
    "#### Steps to Follow:\n",
    "\n",
    "1. **K-Fold Data Splitting:**\n",
    "   - Manually split the dataset into K equal parts (folds). Ensure that each fold is used as a validation set once, while the model is trained on the remaining K-1 folds.\n",
    "   \n",
    "2. **Training with Gradient Descent:**\n",
    "   - For each fold, train the model using gradient descent on the K-1 training folds and validate it on the remaining fold.\n",
    "   - Track the performance metrics (such as MSE) for each fold.\n",
    "\n",
    "3. **Average Performance Metrics:**\n",
    "   - After completing the K iterations, calculate the average performance metrics across all folds. This provides a more generalized estimate of the model’s performance.\n",
    "\n",
    "4. **Learning Curve:**\n",
    "   - Plot a learning curve that shows how the cost function evolves during the training process across different folds. This will illustrate how well the model is learning from the training data over time.\n",
    "\n",
    "### 4. Comparison with Built-in Python Libraries\n",
    "\n",
    "#### Task Overview:\n",
    "You will compare your custom implementations from sections 2 and 3 with equivalent models and validation techniques provided by popular Python libraries, such as scikit-learn.\n",
    "\n",
    "#### Steps to Follow:\n",
    "\n",
    "1. **Use Built-in Functions:**\n",
    "   - Implement multivariate linear regression and K-Fold cross-validation using functions from libraries like scikit-learn.\n",
    "   - Train and validate the model using these built-in functions.\n",
    "\n",
    "2. **Compare Results:**\n",
    "   - Compare the performance metrics (e.g., MSE, R^2) of your custom implementation against the built-in implementations.\n",
    "   - Also, compare the visualizations (predictions vs. actuals, learning curves) produced by both approaches.\n",
    "\n",
    "3. **Report Findings:**\n",
    "   - Provide a detailed report summarizing the differences in performance, any advantages or disadvantages observed in your custom implementation compared to the built-in methods, and possible reasons for these differences.\n",
    "   - Discuss any insights gained from implementing these methods manually versus using libraries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1984ad12",
   "metadata": {},
   "source": [
    "### 1.Hints for Implementation of Simple Linear Regression\n",
    "\n",
    "Main form of simple linear regression function:\n",
    "$$f(x) = \\alpha x + \\beta$$\n",
    "\n",
    "here we want to find the bias ($\\alpha$) and slope($\\beta$) by minimizing the derivation of the Residual Sum of Squares (RSS) function:\n",
    "\n",
    "- step 1: Compute RSS of the training data  \n",
    "\n",
    "$$ RSS = \\Sigma (y_i - (\\hat{\\beta} + \\hat{\\alpha} * x_i) )^2 $$\n",
    "\n",
    "- step 2: Compute the derivatives of the RSS function in terms of $\\alpha$ and $\\beta$, and set them equal to 0 to find the desired parameters\n",
    "\n",
    "$$ \\frac{\\partial RSS}{\\partial \\beta} = \\Sigma (-f(x_i) + \\hat{\\beta} + \\hat{\\alpha} * x_i) = 0$$\n",
    "$$ \\to \\beta = \\hat{y} - \\hat{\\alpha} \\hat{x} \\to (1)$$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial RSS}{\\partial \\alpha} = \\Sigma (-2 x_i y_i + 2 \\hat{\\beta} x_i + 2\\hat{\\alpha} x_i ^ 2) = 0 \\to (2)$$\n",
    "\n",
    "$$ (1) , (2) \\to \\hat{\\alpha} = \\frac{\\Sigma{(x_i - \\hat{x})(y_i - \\hat{y})}}{\\Sigma{(x_i - \\hat{x})^2}}\n",
    "$$\n",
    "$$ \\hat{\\beta} = y - \\hat{a} x$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e49b6da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(input, output):\n",
    "    #TO DO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "169e3f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_regression_predictions(input, intercept, slope):\n",
    "    predicted_values = [(intercept * x) + slope for x in input]\n",
    "    return (predicted_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca25a0e8",
   "metadata": {},
   "source": [
    "### 2. Hints for Implementation of Multivariate Regression from Scratch\n",
    "\n",
    "Multiple regression is a statistical technique that aims to model the relationship between a dependent variable and two or more independent variables.\n",
    "\n",
    "Multiple regression with n independent variables is expressed as follows:\n",
    "\n",
    "$$f(x) = \\beta _{0} + \\beta_{1} x_{1} + \\beta_{2} x_{2} + \\beta_{3} x_{3} + \\beta_{4} x_{4} + ... + \\beta_{n} x_{n} + c $$\n",
    "\n",
    "To optimize the model for accurate predictions, multiple regression commonly employs iterative algorithms such as gradient descent.\n",
    "\n",
    "The main goal of the optimization process is to make our predictions as close as possible to the actual values.\n",
    "We measure the prediction error using a cost function, usually denoted as $J(\\beta)$.\n",
    "\n",
    "$$ J(\\beta)= \\frac {1}{2m} Σ_{i=0}^{m-1}(y_i - (\\hat \\beta _{0} + \\hat \\beta_{1} x_{1} + \\hat \\beta_{2} x_{2} + \\hat \\beta_{3} x_{3} + \\hat \\beta_{4} x_{4} + ... + \\hat \\beta_{n} x_{n}) )^2  $$\n",
    "\n",
    "Gradient descent iteratively adjusts the coefficients $(\\beta_i)$ to minimize the cost function. The update rule for each coefficient is:\n",
    "\n",
    "$$\\beta_{i} = \\beta _ {i} - \\alpha \\frac {∂J(\\beta)}{∂\\beta_{i}}$$\n",
    "\n",
    "$$ \\frac {∂J(\\beta)}{∂\\beta_{i}} = \\frac {1}{m}Σ_{j=0}^{m-1}(y_j - (\\hat \\beta _{0} + \\hat \\beta_{1} x_{j1} + \\hat \\beta_{2} x_{j2} + \\hat \\beta_{3} x_{j3} + \\hat \\beta_{4} x_{j4} + ... + \\hat \\beta_{n} x_{jn})) x_{ji} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "473f56bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_output(feature_matrix, weights, bias):\n",
    "    predictions = np.dot(feature_matrix, weights) + bias\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3805094",
   "metadata": {},
   "source": [
    "**Computing the Derivative**\n",
    "\n",
    "As we saw, the cost function is the sum over the data points of the squared difference between an observed output and a predicted output.\n",
    "\n",
    "Since the derivative of a sum is the sum of the derivatives, we can compute the derivative for a single data point and then sum over data points. We can write the squared difference between the observed output and predicted output for a single point as follows:\n",
    "\n",
    "$$\n",
    "(output  - (const* w _{0} + [feature_1] * w_{1} + ...+ [feature_n] * w_{n}  ))^2\n",
    "$$\n",
    "\n",
    "With n feautures and a const , So the derivative will be :\n",
    "\n",
    "\n",
    "$$\n",
    "2 * (output  - (const* w _{0} + [feature_1] * w_{1} + ...+ [feature_n] * w_{n}  ))\n",
    "$$\n",
    "\n",
    "The term inside the paranethesis is just the error (difference between prediction and output). So we can re-write this as:\n",
    "\n",
    "$$2 * error*[feature_i] $$\n",
    "\n",
    "\n",
    "That is, the derivative for the weight for feature i is the sum (over data points) of 2 times the product of the error and the feature itself. In the case of the constant then this is just twice the sum of the errors!\n",
    "\n",
    "Recall that twice the sum of the product of two vectors is just twice the dot product of the two vectors. Therefore the derivative for the weight for feature_i is just two times the dot product between the values of feature_i and the current errors.\n",
    "\n",
    "\n",
    "With this in mind, complete the following derivative function which computes the derivative of the weight given the value of the feature (over all data points) and the errors (over all data points).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b850b5ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):\n",
    "    #TO DO\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac8fa28",
   "metadata": {},
   "source": [
    "**Gradient Descent**\n",
    "\n",
    "Now we will write a function that performs a gradient descent. The basic premise is simple. Given a starting point we update the current weights by moving in the negative gradient direction. Recall that the gradient is the direction of increase and therefore the negative gradient is the direction of decrease and we're trying to minimize a cost function.\n",
    "\n",
    "\n",
    "The amount by which we move in the negative gradient direction is called the 'step size'. We stop when we are 'sufficiently close' to the optimum. We define this by requiring that the magnitude (length) of the gradient vector to be smaller than a fixed 'tolerance'.\n",
    "\n",
    "\n",
    "With this in mind, complete the following gradient descent function below using your derivative function above. For each step in the gradient descent we update the weight for each feature befofe computing our stopping criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "018f0c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_gradient_descent(feature_matrix, outputs, weights, bias, step_size, tolerance):\n",
    "    converged = False\n",
    "    while not converged:\n",
    "        pass\n",
    "        #TO DO\n",
    "        if np.linalg.norm(gradient) < tolerance:\n",
    "            converged = True\n",
    "\n",
    "    return weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6f3a394a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_features(chosen_features, data_frame):\n",
    "    for feature in chosen_features:\n",
    "        data_frame[feature] = (data_frame[feature] - data_frame[feature].mean()) / data_frame[feature].std()\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "def n_feature_regression(chosen_feature_matrix, target_matrix, keywords):\n",
    "    initial_weights = keywords['initial_weights']\n",
    "    step_size = keywords['step_size']\n",
    "    tolerance = keywords['tolerance']\n",
    "    bias = keywords['bias']\n",
    "    weights = np.array(initial_weights)\n",
    "    weights, bias = regression_gradient_descent(chosen_feature_matrix, target_matrix, weights, bias, step_size,\n",
    "                                                tolerance)\n",
    "\n",
    "    return weights, bias\n",
    "\n",
    "def get_weights_and_bias(chosen_features):\n",
    "\n",
    "    keywords = {\n",
    "        'initial_weights': np.array([.5]*len(chosen_features)),\n",
    "        'step_size': 1.e-4,\n",
    "        'tolerance': 1.e-10,\n",
    "        'bias': 0\n",
    "    }\n",
    "\n",
    "    chosen_feature_dataframe = X_train[chosen_features]\n",
    "    pass\n",
    "    #TO DO\n",
    "    return chosen_feature_matrix, train_weights, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27721bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fd94f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
