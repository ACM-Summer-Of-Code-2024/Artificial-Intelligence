{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "682495bd",
      "metadata": {
        "id": "682495bd"
      },
      "source": [
        "   \n",
        "<div style=\"background-image: linear-gradient(to left, rgb(255, 255, 255), rgb(138, 136, 136)); width: auto; margin: 10px;\">\n",
        "  <img src=\"https://upload.wikimedia.org/wikipedia/en/thumb/f/fd/University_of_Tehran_logo.svg/225px-University_of_Tehran_logo.svg.png\" width=100px width=auto style=\"padding:10px; vertical-align: center;\">\n",
        "\n",
        "</div>\n",
        "   \n",
        "<div   style:\"text-align: center; background-image: linear-gradient(to left, rgb(255, 255, 255), rgb( 219, 204, 245  ));width: 400px; height: 30px; \">\n",
        "<h1 style=\"font-family: Georgia; color: black; text-align: center; \">SOC AI Course </h1>\n",
        "\n",
        "</div>\n",
        "    <div   style:\"border: 3px solid green;text-align: center; \">\n",
        "<h1 style=\"font-family: Georgia; color: black; text-align: center; \">Week2</h1>\n",
        "<h5 style=\"font-family: Georgia; color: black; text-align: center; \">Designers: Mohammad Amanlou, Marzieh Mousavi</h5>\n",
        "mohammadamanlou2@gmail.com,\n",
        "mzmousavi9@gmail.com\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "   </html>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53ca5711",
      "metadata": {
        "id": "53ca5711"
      },
      "source": [
        "### Introduction\n",
        "The objective of this exercise is to become familiar with machine learning methods for predicting housing prices in Boston, USA. This exercise consists of three main sections and one optional section:\n",
        "\n",
        "1. **Introduction to the Dataset:**\n",
        "   In this section, we will familiarize ourselves with the data, including the distribution, types of data, and statistical information related to the dataset. This part is generally referred to as data analytics.\n",
        "\n",
        "2. **Data Preprocessing:**\n",
        "   This section is the most crucial part of any machine learning project. Using the insights from the previous section, we will transform raw real-world data into a suitable format for a machine learning model. This process involves cleaning and summarizing the data.\n",
        "\n",
        "3. **Model Creation and Evaluation:**\n",
        "   In this section, we will create and evaluate various machine learning models. It consists of six phases, starting with manually implementing a first-order Linear Regression model without using pre-built libraries. We will then implement gradient descent and polynomial regression methods, followed by using the scikit-learn library for house price prediction. Subsequent phases will introduce more advanced models, and finally, we will evaluate all models to draw final conclusions.\n",
        "\n",
        "**Note:** Neat implementation of code, use of object-oriented programming in model and function implementation, and organized categorization will be awarded points.\n",
        "\n",
        "### Section 1: Familiarizing with the Dataset\n",
        "The dataset provided contains information about housing prices in Boston along with the characteristics of the houses. Our goal is to analyze these characteristics to understand the criteria and dependencies for predicting house prices.\n",
        "\n",
        "Below is a description of the dataset columns:\n",
        "\n",
        "| Column Name | Description |\n",
        "|-------------|-------------|\n",
        "| CRIM        | Per capita crime rate by town |\n",
        "| ZN          | Proportion of residential land zoned for lots over 25,000 sq.ft. |\n",
        "| INDUS       | Proportion of non-retail business acres per town |\n",
        "| CHAS        | Charles River dummy variable (= 1 if tract bounds river; 0 otherwise) |\n",
        "| NOX         | Nitric oxides concentration (parts per 10 million) |\n",
        "| RM          | Average number of rooms per dwelling |\n",
        "| AGE         | Proportion of owner-occupied units built prior to 1940 |\n",
        "| DIS         | Weighted distances to five Boston employment centers |\n",
        "| RAD         | Index of accessibility to radial highways |\n",
        "| TAX         | Full-value property-tax rate |\n",
        "| PTRATIO     | Pupil-teacher ratio by town |\n",
        "| B           | 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town |\n",
        "| LSTAT       | % lower status of the population |\n",
        "| MEDV        | Median value of owner-occupied homes  |\n",
        "| MEDVC (Target)| Class of Median value of owner-occupied homes  |\n",
        "\n",
        "#### Steps for Initial Data Analysis (EDA):\n",
        "1. Obtain the overall structure of the data using methods like `info` and `describe`.\n",
        "2. Identify missing values in the columns and their proportions.\n",
        "3. Plot the number of unique values for each feature and explain them.\n",
        "4. Plot the Correlation matrix.\n",
        "\n",
        "### Section 2: Data Preprocessing\n",
        "Data preprocessing is essential in any machine learning project. In this phase, we modify the format of the data, clean, and summarize it for model training. This step improves model efficiency and learning speed.\n",
        "\n",
        "#### Preprocessing Steps:\n",
        "0. **create MEDVC:** Divide the house prices into three categories: high, medium, and low, based on a custom criterion. use statistical principles you've learned so far to create these groups and add a new column to the dataset for the corresponding categories.(no for loop is allowed!)\n",
        "\n",
        "1. **Handling Missing Values:** Explain and implement at least three methods to handle missing values, providing reasons for each choice.\n",
        "2. **Feature Removal:** Determine if any columns can be removed and justify the decisions.\n",
        "3. **Categorical vs. Numerical Features:** Identify and differentiate between categorical and numerical features in the dataset.\n",
        "4. **Normalization and Standardization:** Explain the purpose and differences between normalization and standardization for numerical features. Determine if this step is needed for the project.\n",
        "5. **Preprocessing Categorical Features:** Suggest useful preprocessing steps for categorical features typically stored as strings or objects.\n",
        "6. **Data Split:** Research and explain common methods for splitting data into training, validation, and test sets, then apply these methods to the dataset.\n",
        "\n",
        "### Section 3: Model Training, Evaluation, and Tuning\n",
        "\n",
        "#### Phase 1: Classification\n",
        "Implement Decision Trees and Naive Bayes models using scikit-learn, optimize them, and evaluate their performance.\n",
        "\n",
        "1. **Model Implementation:** Implement Decision Trees and Naive Bayes models, tune hyperparameters using GridSearchCV, and optimize to avoid overfitting.\n",
        "2. **Model Evaluation:** Use various evaluation metrics to assess model performance.\n",
        "\n",
        "#### Phase 2: Ensemble Methods\n",
        "Ensemble methods like Bagging and Boosting are used to improve model accuracy by combining multiple simpler models.\n",
        "\n",
        "1. **Hyperparameters:** below are some hyperparameters in sklearn randomforest classifier.explain each of them.choose two of the hyperparameters and explain the impact of them on the model.\n",
        "\n",
        "n_estimators,criterion,max_depth,min_samples_split,min_samples_leaf, max_features, bootstrap\n",
        "\n",
        "2. **Hyperparameter Tuning:** Use GridSearchCV to tune the hyperparameters of a random forest and an XGBoost model and report the best parameters.\n",
        "\n",
        "\n",
        "### Section 4: Model Evaluation\n",
        "Evaluating classification models is crucial to ensure optimal performance.\n",
        "\n",
        "1. **Confusion Matrix:** Explain what a confusion matrix is and its applications and draw it for all models.\n",
        "2. **Evaluation Metrics:** Research Recall, Precision, Accuracy, and F1 Score, explaining their calculation and importance. Compute them for all models."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}